####Chapter 5. Hidden Structures: Clustering, STring Distance, Text Vectors and Topic Modeling####

setwd("C:/Users/dcng/Documents/text_mining-master")
options(stringsAsFactors = FALSE)
Sys.setlocale("LC_ALL", "C")
set.seed(1234)
library(skmeans)
library(tm)   ###텍스트 전처리
library(clue)
library(cluster)   ###clustering에 사용
library(fpc)
library(wordcloud)   ###wordcloud 불러오면 color palette도 같이 불러와진다.


###K-Means Clustering

#data import, 전처리
clean.corpus <- function(corpus) {
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removeWords,
                   c(stopwords("english"), "customer", "service", "customers", "calls"))
  return(corpus)
}

wk.exp <- read.csv("1yr_plus_final4.csv", header=T, encoding="UTF-8")
wk.source <- VCorpus(VectorSource(wk.exp$text))
wk.corpus <- clean.corpus(wk.source)
wk.dtm <- DocumentTermMatrix(wk.corpus, 
                             control=list(weighting=weightTfIdf))
#         weightTf : 문장 내에서 단어가 몇번이나 등장하는지
#         weightTfIdf : 문서내에서 등장한 단어가 다른 문서에서도 많이 나오면
#                       그렇게 중요하지 않은 단어로 가중치를 부과함.
#                    갑자기 한 문서에서 특정 단어의 빈도가 높게 나오면 다른 문서들과는 다른 특성을 가진 것으로 본다.


#정규화
wk.dtm.s <- scale(wk.dtm, scale=TRUE)  ##숫자가 들쑥날쑥한 numeric상태이므로 정규화함.

#k-means clustering
wk.clusters <- kmeans(wk.dtm.s, 3)  #cluster 갯수가 3개

#barplot으로 clustering 분포 확인하기
barplot(wk.clusters$size, main='k-means')   #매우 불균형한 상ㅌ

#cluster 확인하는 첫번째 방법 - plotcluster 함수 사용
plotcluster(cmdscale(dist(wk.dtm)), wk.clusters$cluster)태
#cmdscale=다차원 척도법: 변수가 두개 이상인 경우 데이터 사이의 관계를 저차원 공간에 점으로 표현하는 것

#cluster 확인하는 두번째 방법- silhouette plot 그리기 -각 partition의 shadow를 보여줌.
dissimilarity.m <- dist(wk.dtm.s)
plot(silhouette(wk.clusters$cluster, dissimilarity.m))
#이 모델의 경우 clustering이 제대로 되지 않아서 한 partition이 거의 전부를 차지한다.


##silhouette plot
#한줄한줄이 document를 의미함.
#Average silhouette width: 군집화가 잘 된 정도. 1이면 각 cluster안의 문서들이 다른 cluster로 갈 여지가 아예 없는 상태
#                          보통 0.5이 넘어야 어느정도 제대로 clustering되었다고 할 수 있음.
#cluster안의 문서가 다른 cluster로 갈 확률: 1-오른쪽에 써있는 확률


#extract the prototypical words for each cluster - clue library의 cl_prototypes 함수
work.clus.proto <- t(cl_prototypes(wk.clusters))
#comparison cloud로 visualize
comparison.cloud(work.clus.proto, max.words=100)
#두번째 cluster는 unique word가 없음 => 2번째 cluster에 나온 단어들은 1,3번째에도 다 있다



###Spherical K-Means Clustering
library(skmeans)
library(clue)
wk.dtm <- DocumentTermMatrix(wk.corpus, control=list(weighting=weightTfIdf))

soft.part <- skmeans(wk.dtm, 3, m=1.2, control=list(nruns=5, verbose=TRUE))

#visualization
barplot(table(soft.part$cluster), main="Spherical K-means")
plotcluster(cmdscale(dist(wk.dtm)), soft.part$cluster)
plot(silhouette(soft.part))

#extract prototype score, visualize by comparison cloud
s.clus.proto <- t(cl_prototypes(soft.part))
comparison.cloud(s.clus.proto, max.words = 100)

#각  cluster별 top five most prototypial terms 확인하기
sort(s.clus.proto[,1], decreasing = TRUE)[1:5]
sort(s.clus.proto[,2], decreasing = TRUE)[1:5]
sort(s.clus.proto[,3], decreasing = TRUE)[1:5]




###K-Mediod Clustering    -- centroids 대신 median으로 구하는 mediods가 cluster center의 명칭
##   특징 1. actual prototypical center를 가짐.
##   특징 2. k-means보다 outlier에 훨씬 영향을 덜 받음.림
##   특징 3. 각 중심점끼리의 거리를 계산해야돼서 속도가 느리

wk.dtm <- DocumentTermMatrix(wk.corpus, control=list(weighting=weightTfIdf))

#fpc library의 pamk function: k-mediods object를 형성
wk.mediods <- pamk(wk.dtm, krange=2:4, critout=TRUE)   ##krange : clusterr갯수를 2~4개 사이에서 지정. silhouette area가 가장 높은 k로 선정됨.

dissimilarity.m <- dist(wk.dtm)
plot(silhouette(wk.mediods$pamobject$clustering, dissimilarity.m))



###Evaluating the Cluster Approaches
#comparison of cluster output : cluster.stats 함수 이용(fpc package)
results <- cluster.stats(dist(wk.dtm.s), wk.clusters$cluster, wk.mediods$pamobject$clustering)
#실루엣 분포의 면적 등이 evaluation에 많이 쓰임.



###Calculating and Exploring String Distance  한 문자열에서 다른 문자열까지의 거리.
#                                             검색 알고리즘을 만들 때 사용함.

library(stringdist)
stringdist('crabapple', 'apple', method="lcs")  #상위문자열(apple)에서 하위문자열의 갯수를 뺀 값을 반환
stringdist('crabapples', 'apple', method="lcs")


#Fuzzy Matching - Amatch, Ain
#Amatch 함수: straightforward fuzzy match. match되는 위치를 알려줌
match('apple', c('crabapple', 'pear'))   #아예 똑같아야댐
amatch('apple', c('crabapple', 'pear'), maxDist=3, method='dl')
amatch('apple', c('crabapple', 'pear'), maxDist=4, method='dl')  #벡터의 첫번째 값인 crabapple과 연관이 있음

#ain 함수: match되는지 안되는지 T/F로 알려줌.
ain('raspberry', c('berry', 'pear'), maxDist=4, method='dl')


##Similarity Distances :actual minimum number of operators를 구해줌.
#   stringdist 함수: 하나또는 한개 이상의 number of operator를 구함
#   stringdistmatrix 함수: 모든 string의 number of operator를 구함

stringdist('raspberry', c('berry', 'pear'), method='hamming')   #hamming은 deletion을 허용 안하므로 둘다 inf
stringdist('raspberry', c('berry', 'pear'), method='osa')

fruit <- c('crabapple', 'apple', 'raspberry')
fruit.dist <- stringdistmatrix(fruit)
fruit.dist

a <- c("aaa", "aab", "bbc", "ccc")
b <- c("abc", "cab", "bca", "ccc")
d <- stringdistmatrix(a,b)
d

#dendrogram
plot(hclust(fruit.dist), labels=fruit)




###LDA topic Modeling
#   다양한 토픽들 중 특정단어가 어떤 topic에 속하는가?
#   한 문서 안에서 각 topic들의 비중은? (가장 큰 비중을 가진 토픽이 메인 토픽이 된다)
#   'Gibbs Sampling'을 바탕으로/ 다른 단어들의 topic들의 확률을 바탕으로 topic을 할당함
library(qdap)
library(lda)
library(GuardianR)
library(pbapply)
library(LDAvis)
library(treemap)
library(car)

text <- read.csv("Guardian_articles_11_14_2015_12_1_2015.csv")
# LDA는 corpus대신 vector를 사용하므로 corpus로 전처리할 필요가 없음.

#single vector 전처리
articles <- iconv(text$body, "latin1", "ASCII", sub="") #문자벡터의 ecncoding 바꾸기. ASCII가 영어라고 합니다
articles <- gsub('http\\S+\\s*', '', articles)  #역슬래시 두개:정규방정식. \\S는 빈칸이 아님을 의미함. +는 한번이상 매칭시켜라는 의미.
                                                #반대로 \\s는 공백을 의미함!
articles <- bracketX(articles, bracket = 'all')  #괄호안의 문자 제거
articles <- gsub('[[:punct:]]', "", articles)
articles <- removeNumbers(articles)
articles <- tolower(articles)
articles <- removeWords(articles, c(stopwords('en'), 'pakistan', 'gmt', 'england'))

#빈칸인 string은 제거하는 함수 만들기
#  (LDA는 공백으로 이루어진 덩어리를 단어로 인식하므로 이 문제를 해결하기 위함)
blank.removal <- function(x) {
  x <- unlist(strsplit(x,' '))
  x <- subset(x, nchar(x)>0)
  x <- paste(x, collapse=' ')
}


##string 제거 함수 이해 ##
ex.text <- c("Text mining is a      good time", "Text mining is a good time")
strsplit(ex.text[1], ' ')
strsplit(ex.text[2], ' ')
#띄어쓰기마다로 나누기
char.vec <- unlist(strsplit(ex.text[1], ' '))  #strsplit함수가 list를 반환하므로 unlist함수를 씀.
char.vec
#한글자 이상 있는 것만 살리기
char.vec <- subset(char.vec, nchar(char.vec)>0)
char.vec
#paste로 다시 붙여서 큰 공백없는 문장 만들기
char.vec <- paste(char.vec, collapse=" ")
char.vec


atricles <- pblapply(articles, blank.removal)
#lapply도 되긴 되는데, pblapply는 진행속도를 보여줌.


#lda package의 lexicalize 함수를 이용한 feature extraction
#  lexicalize를 통해 documents와 vocab을 얻을 수 있음.
ex.text <- c('this is a text document')
ex.text.lex <- lexicalize(ex.text)
ex.text.lex$documents[[1]]  

ex.text.lex$vocab

ex.text <- c('this is a document', 'text mining a text document is great')
ex.text.lex <- lexicalize(ex.text)
ex.text.lex$documents  #각 문서의 word frequency 행렬을 리스트로 반환함.
ex.text.lex$vocab   #문서 전체의 unique한 단어들 (중복 단어는 제거됨)

documents <-lexicalize(articles)
wc <- word.counts(documents$documents, documents$vocab) #특정 단어가 문서 전체에서 몇 번 사용 되었는지?
doc.length <- document.lengths(documents$documents) #문서별 단어의 총 개수.(문서의 크기)
head(wc) 


k <- 4
num.iter <- 25
alpha <- 0.02
eta <- 0.02
set.seed(1234)
fit <- lda.collapsed.gibbs.sampler(documents = documents$documents, K=k, vocab=documents$vocab,
                                   num.iterations = 25, alpha=0.02, eta=0.02, initial=NULL,  #NULL:random으로 토픽을 할당하고 시작
                                   burnin=0, compute.log.likelihood = TRUE)

plot(fit$log.likelihoods[1,]) #로그우도값

#토픽을 대표하는 단어들       fit$topics : 그 단어가 각 topic에 할당된 총 횟수
top.topic.words(fit$topics, 7, by.score=TRUE)
#  이 함수를 통해 각 토픽의 단어들을 보고 토픽의 이름을 대충 붙여볼 수 있다.


#각 토픽에 가장 잘 어울리는 문서 번호(위치)     fit$document_sums : 한 문서에서 각 토픽에 할당된 단어의 수
top.topic.documents(fit$document_sums, 3)

theta <- t(pbapply(fit$document_sums+alpha, 2, function(x) x/sum(x)))   #문서당 각 토픽의 비중
phi <- t(pbapply(t(fit$topics)+eta, 2, function(x) x/sum(x)))   #토픽 내 단어들의 비중

#LDA visualization(상호작용 시각화)
article.json <- createJSON(phi=phi, theta=theta, doc.length = doc.length, 
                           vocab=documents$vocab, term.frequency = as.vector(wc))
serVis(articls.json)  #분포 그리기

#Treemap visualization
doc.assignment <- function(x) {
  x <- table(x)
  x <- as.matrix(x)
  x <- t(x)
  x <- max.col(x)
}

fit$assignments[[2]][1:10] #문서의 각 단어에 할당된 topic
table(fit$assignments[[2]][1:10]) #각 topic에 할당된 단어의 갯수
t(as.matrix(table(fit$assignments[[2]][1:10])))
max.col(t(as.matrix(table(fit$assignments[[2]][1:10])))) #해당 문서의 메인토픽

assignments <- unlist(pblapply(fit$assignments, doc.assignment))

assignments <- recode(assignments, "1='Criket1'; 2='Paris Attacks'; 3='Cricket2'; 4='Unknown'") #토픽이름지정

article.ref <- seq(1:nrow(text))  #문서의 번호 지정
article.pol <- polarity(articles)[[1]][3]   #모든 문서들의 polarity 계산
article.tree.df <- cbind(article.ref, article.pol, doc.length, assignments)
treemap(article.tree.df, index=c('assignments', 'article.ref'),
        vSize="doc.length", vColor = "polarity", type="value", 
        title="Guardan Articles mentioning Pakistan",
        palette=c("red", "white", "green"))
