setwd("C:/Users/dcng/Documents/text_mining-master")
Sys.setlocale("LC_ALL", "C")
library(tm)
library(Matrix)
library(glmnet)
library(caret)
library(pROC)
library(ggthemes)
library(ggplot2)
library(arm)

#create a preprocessing custom function
headline.clean <- function(x) {
  x <- tolower(x)
  x <- removeWords(x, stopwords('en'))
  x <- removePunctuation(x)
  x <- stripWhitespace(x)
  return(x)
}


#match.matrix 함수 만들기: original matrix를 참고하여 new DTM 생성
match.matrix <- function(text.col, original.matrix=NULL, weighting=weightTf){
  control <- list(weighting=weighting)
  training.col <-
    sapply(as.vector(text.col, mode="character"), iconv, to="UTF8", sub="byte")
  corpus <- VCorpus(VectorSource(training.col))
  matrix <- DocumentTermMatrix(corpus, control=control);
  if (!is.null(original.matrix)) {
    terms <- colnames(original.matrix[,
                                      which(!colnames(original.matrix) %in% colnames(matrix))])
    weight <- 0
    if (attr(original.matrix, "weighting")[2] =="tfidf") 
      weight <- 0.000000001
    amat <- matrix(weight, nrow=nrow(matrix), ncol=length(terms))
    colnames(amat) <- terms
    rownames(amat) <- rownames(matrix)
    fixed <- as.DocumentTermMatrix(
      cbind(matrix[,which(colnames(matrix) %in% colnames(original.matrix))], amat),
      weighting = weighting)
    matrix <- fixed
  }
  matrix <- matrix[,sort(colnames(matrix))]
  gc()
  return(matrix)
}



#데이터 불러오기 / train set과 test set 나누기
raw.headlines<-readLines('https://raw.githubusercontent.com/kwartler/text_mining/master/all_3k_headlines.csv',
                         encoding='latin1')[-1]
raw.headlines[2969]<-paste(gsub('\\"','',raw.headlines[2969]),
                           gsub('\\",','',raw.headlines[2971]),
                           sep=',')
raw.headlines<-raw.headlines[-c(2970,2971)]
library(stringr)
headline_etc<-str_split(raw.headlines, 'http{1}', simplify=TRUE)
headline_etc[1513,2]<-paste(headline_etc[1513,2],
                            'http',headline_etc[1513,3],
                            sep='')
text<-headline_etc[,1]
url_site_y<-paste('http',headline_etc[,2],sep='')
url_site_y<-str_split(url_site_y,',',simplify=TRUE)
url<-url_site_y[,1]
site<-url_site_y[,2]
y<-url_site_y[,3]
headlines<-tibble(
  headline=text,
  url=url,
  site=site,
  y=factor(y)
)

train <- createDataPartition(headlines$y, p=0.5, list=FALSE)
train.headlines <- headlines[train,]
test.headlines <- headlines[-train,]

#전처리
clean.train <- headline.clean(train.headlines$headline)
#dtm 생성
train.dtm <- match.matrix(clean.train,weighting=weightTfIdf)

train.dtm

train.matrix <- as.matrix(train.dtm)
train.matrix <- Matrix(train.matrix, sparse=TRUE) #sparse matrix로 만듦.

#train.dtm과 sparse matrix인 train.matrix 비교
dim(train.matrix)
train.matrix[1:5, 1:25]

#cross validation
cv <- cv.glmnet(train.matrix, y=as.factor(train.headlines$y), alpha=1,
                family="binomial", nfolds = 10, intercept=F, type.measure = 'class')

plot(cv)


#use classifier model for model's accuracy
pred <-predict(cv, train.matrix, type='class', s=cv$lambda.1se)

#ROC curve(receiver operator characteristics curve) : AUC값 구하기
train.auc <- roc(train.headlines$y, as.numeric(pred))
train.auc
plot(train.auc)

#confusion matrix로 정확성 확인
confusion <- table(pred, train.headlines$y)
confusion
sum(diag(confusion))/sum(confusion) #accuracy


## Making predictions

#test data set을 train set과 똑같이 전처리하고 colnames 맞추기
clean.test <- headline.clean(test.headlines$headline)
test.dtm <- match.matrix(clean.test, weighting = weightTfIdf, original.matrix = train.dtm)
#test.dtm과 train.dtm 비교 

test.dtm ; train.dtm

test.matrix <- as.matrix(test.dtm)
test.matrix <- Matrix(test.matrix)

preds <- predict(cv, test.matrix, type='class', s=cv$lambda.min)
headline.preds <- data.frame(doc_row=rownames(test.headlines), class=preds[,1])

#Test set evaluation
test.auc <- roc(test.headlines$y, as.numeric(preds))
test.auc
plot(train.auc, col="blue", main="RED=test, BLUE=train", adj=0)
plot(test.auc, add=TRUE, col="red", lty=2)

cnfusion <- table(headline.preds[,2], test.headlines$y)
sum(diag(confusion))/sum(confusion)

#Finding most impactful words
glmnet.coef <- as.matrix(coef(cv, s='lambda.min'))
glmnet.coef <- data.frame(words=row.names(glmnet.coef),
                          glmnet_coefficients=glmnet.coef[,1])

glmnet.coef <- glmnet.coef[order(
  glmnet.coef$glmnet_coefficients, decreasing = T),]
glmnet.coef$words <- factor(glmnet.coef$words, levels=unique(glmnet.coef$words))

summary(glmnet.coef$glmnet_coefficients)

length(subset(glmnet.coef$glmnet_coefficients, glmnet.coef$glmnet_coefficients>0))

length(subset(glmnet.coef$glmnet_coefficients, glmnet.coef$glmnet_coefficients<0))
ggplot(glmnet.coef, aes(glmnet.coef$glmnet_coefficients))+
  geom_line(stat='density', color='darkred', size=1)+theme_gdocs()

top.coef <- rbind(head(glmnet.coef, 10),
                  tail(glmnet.coef, 10))
top.coef$impact <- ifelse(
  top.coef$glmnet_coefficients>0, "Positive", "Negative")

ggplot(top.coef, aes(glmnet_coefficients, words))+
  geom_segment(aes(yend=words), xend=0, colour="grey50")+
  geom_point(size=3, aes(colour=impact))+theme_few()


#arm 패키지의 invlogit function
glmnet.coef$probability <- invlogit(
  glmnet.coef$glmnet_coefficients)
top.coef$probability <- invlogit(top.coef$glmnet_coefficients)

plot(glmnet.coef$probability, glmnet.coef$word, col='blue')
plot(top.coef$probability, top.coef$word, col='red', pch=16)
